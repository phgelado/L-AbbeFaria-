


pip install requests beautifulsoup4


import requests
from bs4 import BeautifulSoup

# URL of the webpage you want to scrape
url = 'https://www.mrmoneymustache.com/2011/05/18/how-to-make-money-in-the-stock-market/'

# Send a GET request to the webpage
response = requests.get(url)

# Initialize an empty string to hold all the text
all_text = ""

# Check if the request was successful
if response.status_code == 200:
    # Parse the content of the request with BeautifulSoup
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Extract all text within paragraph tags
    paragraphs = soup.find_all('p')
    for paragraph in paragraphs:
        # Add each paragraph's text to the all_text string with a space in between
        all_text += paragraph.text.strip() + " "
    
    # Optional: print the complete concatenated text
    #print(all_text)
else:
    print(f"Failed to retrieve the webpage. Status code: {response.status_code}")



import ollama


from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
max_tokens = 1024  # Adjust based on your model's max context length
overlap = max_tokens // 4  # Calculate overlap as half of max_tokens

def chunk_text(text, max_tokens, overlap):
    tokens = tokenizer.encode(text)
    chunks = []
    for i in range(0, len(tokens), overlap):
        chunk = tokens[i:i + max_tokens]
        chunks.append(chunk)
        if len(chunk) < max_tokens:
            break
    return [tokenizer.decode(chunk) for chunk in chunks]

text = all_text
chunks = chunk_text(text, max_tokens, overlap)


from string import Template


prompt_creator = Template(""" You are a curriculum creator. You teach others by creating question and answer pairs from documents. Your focus is distilling
pure knowledge by extracting generalisable, deep underlying principles, this means that you ignore information that is only relevant to a 
particular document, and focus on knowledge contained or described in the text you read. 

Your task is to teach about financial advice, and you extract key pieces of information that are underlying principles on personal finance, financial advice, the economy and life philosophy in general.  

You care about definitions, principles, rules, ways of thinking and ways of approaching and solving problems.

Please create question and answer pairs from the following excerpt.
**Document:**
$text
""")


prompt_supervisor = Template("""You are a content supervisor that decides on a question answer pair currciculum for a personal finance course. 
You have just received a series of question answer pairs from a document.

Your task is to review them, accepting them or improving them. 

You first will provide you reasoning, and improved version if necessary.

You will then respond with a new section, titled "Question Answer Pairs", where you will place the final version of these.

**Document**
$text

**Question answer pairs:**
$previous
""")


prompt_extractor = Template("""Please extract the final relevant question answer pairs into a list of dictionaries that have the form:
[{{'question':question_here, 'answer':answer_here}}, {{'question':question2_here, 'answer':answer2_here}}]

You do not reply anything else, as that would make the list unparsable and your output useless.

**Question answer pairs:**
$previous
""")


prompts = [prompt_creator, prompt_supervisor, prompt_extractor]


def get_ollama_response(prompt):
    response = ollama.chat(model='llama3', messages=[
          {
            'role': 'user',
            'content': prompt,
          },
        ])

    return response['message']['content']


import string
import ast


def pipeline(text, prompts):

    previous = get_ollama_response(prompts[0].substitute(text=text))
    previous = get_ollama_response(prompts[1].substitute(text=text, previous=previous))
    previous = get_ollama_response(prompts[2].substitute(text=text, previous=previous))

    #return previous 
    
    try:
        previous = ast.literal_eval(previous)
    except:
        previous = None
        
    return previous


chunks[0]


pipeline(chunks[0], prompts)


%%time

total_list = []

for chunk in chunks:
    output = pipeline(chunk, prompts)
    if output:
       total_list.append(output)
    else:
        output = pipeline(chunk, prompts)
        if output:
           total_list.append(output)
        else:
            pass


len(total_list)


chunks[3]


%%time
response = ollama.chat(model='llama3', messages=[
  {
    'role': 'user',
    'content': prompt,
  },
])
print(response['message']['content'])


%%time
response = ollama.chat(model='llama3', messages=[
  {
    'role': 'user',
    'content': prompt,
  },
])
print(response['message']['content'])


%%time
response = ollama.chat(model='llama3', messages=[
  {
    'role': 'user',
    'content': prompt,
  },
])
print(response['message']['content'])





import torch


torch.cuda.is_available()


pip install PyMuPDF



import fitz

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    full_text = ""
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        full_text += page.get_text()
    return full_text

# Path to your PDF file
pdf_path = "text.pdf"

# Extract text from the PDF
pdf_text = extract_text_from_pdf(pdf_path)

# Chunk the extracted text
chunks = chunk_text(pdf_text, max_tokens, overlap)


len(chunks)


%%time

total_list = []

for chunk in chunks:
    output = pipeline(chunk, prompts)
    if output:
       total_list.append(output)
    else:
        output = pipeline(chunk, prompts)
        if output:
           total_list.append(output)
        else:
            pass


len(total_list)


total_list[546]


import json

file_name = 'data.json'

with open(file_name, 'w') as json_file:
    json.dump(total_list, json_file, indent=4)



